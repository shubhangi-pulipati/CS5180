{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhDXuXYk_gh1"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow numpy matplotlib gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7Y08-TMqUfx"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E-VVBUYqdFu"
      },
      "outputs": [],
      "source": [
        "# Define custom traffic signal environment\n",
        "class TrafficSignalEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(TrafficSignalEnv, self).__init__()\n",
        "        # Define state space\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(3, 3), dtype=np.uint8)\n",
        "        # Define action space\n",
        "        self.action_space = gym.spaces.Discrete(4)\n",
        "        # Initialize state\n",
        "        self.state = np.zeros((3, 3), dtype=np.uint8)\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset state to initial state\n",
        "        self.state = np.zeros((3, 3), dtype=np.uint8)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Update state based on action (simplified for demonstration purposes)\n",
        "        if action == 0:\n",
        "            self.state[0][0] += 1\n",
        "        elif action == 1:\n",
        "            self.state[0][1] += 1\n",
        "        elif action == 2:\n",
        "            self.state[1][0] += 1\n",
        "        elif action == 3:\n",
        "            self.state[1][1] += 1\n",
        "\n",
        "        reward = np.sum(self.state)\n",
        "        done = False\n",
        "        info = {}\n",
        "        return self.state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fRC-2HXqmru"
      },
      "outputs": [],
      "source": [
        "\n",
        "# DQNAgent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_shape, action_space, learning_rate=0.001, epsilon=1.0, epsilon_decay=0.995, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_space = action_space\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model()\n",
        "        self.memory = deque(maxlen=1000)\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential([\n",
        "            Flatten(input_shape=self.state_shape),\n",
        "            Dense(24, activation='relu'),\n",
        "            Dense(24, activation='relu'),\n",
        "            Dense(self.action_space.n)\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return self.action_space.sample()  # Explore\n",
        "        else:\n",
        "            q_values = self.model.predict(state[np.newaxis, :])  # Exploit\n",
        "            return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = []\n",
        "        targets = []\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis, :])[0])\n",
        "            target_f = self.model.predict(state[np.newaxis, :])\n",
        "            target_f[0][action] = target\n",
        "            states.append(state)\n",
        "            targets.append(target_f)\n",
        "\n",
        "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self, env, episodes=1000, batch_size=32):\n",
        "        total_rewards = []\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                self.replay(batch_size)\n",
        "                self.update_epsilon()\n",
        "            total_rewards.append(total_reward)\n",
        "        return total_rewards"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, action_space, learning_rate=0.1, epsilon=1.0, epsilon_decay=0.995, gamma=0.99):\n",
        "        self.Q = {}  # Q-table\n",
        "        self.action_space = action_space\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return self.action_space.sample()  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.Q.get(tuple(state), [0] * self.action_space.n))  # Exploit\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        state = tuple(state)\n",
        "        next_state = tuple(next_state) if next_state is not None else None\n",
        "        if state not in self.Q:\n",
        "            self.Q[state] = [0] * self.action_space.n\n",
        "        if next_state is not None and next_state not in self.Q:\n",
        "            self.Q[next_state] = [0] * self.action_space.n\n",
        "        old_q_value = self.Q[state][action]\n",
        "        next_max_q = np.max(self.Q[next_state]) if next_state is not None else 0\n",
        "        new_q_value = (1 - self.learning_rate) * old_q_value + self.learning_rate * (reward + self.gamma * next_max_q)\n",
        "        self.Q[state][action] = new_q_value\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "6J4qomKz_Pn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3zkcw8Aqusb"
      },
      "outputs": [],
      "source": [
        "# Create environment and agents\n",
        "env = TrafficSignalEnv()\n",
        "dqn_agent = DQNAgent(env.observation_space.shape, env.action_space)\n",
        "q_learning_agent = QLearningAgent(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7z4eIRfq5w5"
      },
      "outputs": [],
      "source": [
        "# Train agents and collect rewards\n",
        "num_episodes = 100\n",
        "dqn_rewards = dqn_agent.train(env, episodes=num_episodes)\n",
        "q_learning_rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action_dqn = dqn_agent.choose_action(state)\n",
        "        action_q_learning = q_learning_agent.choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action_dqn)\n",
        "        dqn_agent.update_q_table(state, action_dqn, reward, next_state)\n",
        "        q_learning_agent.update_q_table(state, action_q_learning, reward, next_state)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "    q_learning_rewards.append(total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "avg_dqn_reward = sum(dqn_rewards) / len(dqn_rewards)\n",
        "avg_q_learning_reward = sum(q_learning_rewards) / len(q_learning_rewards)\n",
        "\n",
        "print(f\"Average reward for DQN agent: {avg_dqn_reward}\")\n",
        "print(f\"Average reward for Q-Learning agent: {avg_q_learning_reward}\")"
      ],
      "metadata": {
        "id": "Yi5LK9ITuG3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}